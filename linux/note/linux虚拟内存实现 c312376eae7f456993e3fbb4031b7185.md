# linux虚拟内存实现

# 为什么要存在虚拟地址

如果没有虚拟内存地址映射，我们在程序中对内存的操作就全部都是使用物理地址，这种情况下程序员要精确定位每一个变量在内存中的位置，还要考虑内存大小分配的问题等等，这将导致多进程调度运行成为灾难

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled.png)

所以虚拟内存的引入就是要解决上述问题：每个进程都拥有自己独立的虚拟地址空间，进程与进程之间的虚拟内存地址空间是相互独立的

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%201.png)

当CPU访问进程的虚拟地址时，经过地址翻译硬件将虚拟地址转换成不同的物理地址，这样不同进程在运行时，虽然操作的是同一块虚拟地址，但背后映射的是不同的物理地址

进程的虚拟内存视图：

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%202.png)

# 进程虚拟内存管理

## 进程虚拟内存的分配

在Linux中进程的管理是用task_struct结构来管理的：

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%203.png)

在进程描述符结构中，有一个专门描述进程虚拟地址空间的内存描述符mm_struct结构，这个结构体中包含了前边所将的进程虚拟内存空间的全部信息。每个进程都有唯一的mm_struct结构体，当我们调用fork()函数创建一个进程时，表示进程的地址空间mm_struct结构会随着进程描述符的创建而创建

```c
long _do_fork(unsigned long clone_flags,
       unsigned long stack_start,
       unsigned long stack_size,
       int __user *parent_tidptr,
       int __user *child_tidptr,
       unsigned long tls)
{
        ......... 省略 ..........
 struct pid *pid;
 struct task_struct *p;

        ......... 省略 ..........
    // 为进程创建 task_struct 结构，用父进程的资源填充 task_struct 信息
 p = copy_process(clone_flags, stack_start, stack_size,
    child_tidptr, NULL, trace, tls, NUMA_NO_NODE);

         ......... 省略 ..........
}
```

随后会在copy_process函数中创建task_struct结构，并拷贝父进程的相关资源到新进程的task_struct结构中，其中就包含拷贝父进程的虚拟内存空间mm_struct结构，这里可以看出子进程在新创建出来之后它的虚拟空间时和父进程的一样

```c
static __latent_entropy struct task_struct *copy_process(
     unsigned long clone_flags,
     unsigned long stack_start,
     unsigned long stack_size,
     int __user *child_tidptr,
     struct pid *pid,
     int trace,
     unsigned long tls,
     int node)
{

    struct task_struct *p;
    // 创建 task_struct 结构
    p = dup_task_struct(current, node);

        ....... 初始化子进程 ...........

        ....... 开始继承拷贝父进程资源  .......      
    // 继承父进程打开的文件描述符
 retval = copy_files(clone_flags, p);
    // 继承父进程所属的文件系统
 retval = copy_fs(clone_flags, p);
    // 继承父进程注册的信号以及信号处理函数
 retval = copy_sighand(clone_flags, p);
 retval = copy_signal(clone_flags, p);
    // 继承父进程的虚拟内存空间
 retval = copy_mm(clone_flags, p);
    // 继承父进程的 namespaces
 retval = copy_namespaces(clone_flags, p);
    // 继承父进程的 IO 信息
 retval = copy_io(clone_flags, p);

      ...........省略.........
    // 分配 CPU
    retval = sched_fork(clone_flags, p);
    // 分配 pid
    pid = alloc_pid(p->nsproxy->pid_ns_for_children);

.     ..........省略.........
}
```

我们可以重点关注copy_mm函数，这个函数完成了对子进程虚拟内存空间结构体的创建和初始化

```c
static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
{
 // 子进程虚拟内存空间，父进程虚拟内存空间
 struct mm_struct *mm, *oldmm;
 int retval;

        ...... 省略 ......

 tsk->mm = NULL;
 tsk->active_mm = NULL;
    // 获取父进程虚拟内存空间
 oldmm = current->mm;
 if (!oldmm)
  return 0;

        ...... 省略 ......
 // 通过 vfork 或者 clone 系统调用创建出的子进程（线程）和父进程共享虚拟内存空间
 if (clone_flags & CLONE_VM) {
        // 增加父进程虚拟地址空间的引用计数
        mmget(oldmm);
        // 直接将父进程的虚拟内存空间赋值给子进程（线程）
        // 线程共享其所属进程的虚拟内存空间
        mm = oldmm;
        goto good_mm;
 }

 retval = -ENOMEM;
 // 如果是 fork 系统调用创建出的子进程，则将父进程的虚拟内存空间以及相关页表拷贝到子进程中的 mm_struct 结构中。
 mm = dup_mm(tsk);
 if (!mm)
  goto fail_nomem;

good_mm:
 // 将拷贝出来的父进程虚拟内存空间 mm_struct 赋值给子进程
 tsk->mm = mm;
 tsk->active_mm = mm;
 return 0;

        ...... 省略 ......
```

cpoy_mm函数首先会将父进程的虚拟内存空间current→mm赋值给指针oldmm，然后通过dup_mm函数将父进程的虚拟内存空间以及相关页表拷贝到子进程的mm_struct结构中，最后将拷贝出来的mm_struct赋值给子进程的task_struct结构

- 通过fork()函数创建出的子进程，它的虚拟内存空间以及相关页表相当于父进程虚拟内存空间的一份拷贝，直接从父进程中拷贝到子进程中
    
    

当我们通过vfork()或者clone()系统调用创建出的子进程，首先会设置CLONE_VM标识，这样来到copy_mm函数中就会进入if(clone_flags & CLONE_VM)中，在这个分支中会将父进程的虚拟内存空间以及相关页表直接赋值给子进程，这样一来父进程和子进程的虚拟内存空间就变成共享的

子进程共享了父进程的虚拟内存空间，这样子进程就变成了我们熟悉的线程，是否共享地址空间几乎是进程和线程之间的本质区别，linux内核并不特别区分进程和线程，线程对于内核来说仅仅是一个共享特定资源的进程而已

内核线程和用户态线程的区别就是内核线程没有相关的内存描述符mm_struct，内核线程对应的task_struct结构中的mm域指向NULL，所以内核线程之间的调度是不涉及地址空间切换的

当一个内核线程被调度时，它会发现自己的虚拟地址空间指向NULL，虽然它不会访问用户态的内存，但是它会访问内核内存，内核会将调度之前的上一个用户态进程的虚拟内存空间mm_struct直接赋值给内核线程，因为内核线程不会访问用户空间的内存，它仅仅访问内核空间的内存，所以直接复用上一个用户态进程的虚拟地址空间就可以避免为内核线程分配mm_struct和相关页表的开销

## 区分用户态和内核态的地址空间

这个区分用到了进程的内存描述符mm_struct结构体中的task_size变量，这个变量定义了用户态地址空间与内核态地址空间之间的分界线

```c
struct mm_struct {
    unsigned long task_size; /* size of task vm space */
}
```

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%204.png)

## 内核如何布局进程虚拟内存空间

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%205.png)

内核中采用了一个称为内存描述符的mm_struct结构体来标识进程虚拟内存空间的全部信息：

```c
struct mm_struct {
    unsigned long task_size;    /* size of task vm space */
    unsigned long start_code, end_code, start_data, end_data;
    unsigned long start_brk, brk, start_stack;
    unsigned long arg_start, arg_end, env_start, env_end;
    unsigned long mmap_base;  /* base of mmap area */
    unsigned long total_vm;    /* Total pages mapped */
    unsigned long locked_vm;  /* Pages that have PG_mlocked set */
    unsigned long pinned_vm;  /* Refcount permanently increased */
    unsigned long data_vm;    /* VM_WRITE & ~VM_SHARED & ~VM_STACK */
    unsigned long exec_vm;    /* VM_EXEC & ~VM_WRITE & ~VM_STACK */
    unsigned long stack_vm;    /* VM_STACK */

       ...... 省略 ........
}
```

- start_code和end_code定义代码段的起始位置和结束位置，程序编译后的二进制文件中的机器码就存储在这里
- start_data和end_data定义数据段的起始和结束位置，二进制文件中存储的全局变量和静态变量就存储在这里
- start_brk时堆区的标识（我们使用malloc申请小内存的时候，就是通过改变brk的位置来调整堆的大小）

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%206.png)

在mm_struct结构体中除了上述用于划分虚拟内存区域的变量之外，还定义了一些虚拟内存与物理内存映射的内容，OS会将内存划分成一页一页的区域来管理

当内存吃紧的时候，有些页可以被换出到硬盘上：

- locked_vm：被锁定不能换出的内存页的总数
- pinned_vm：既不能换出，也不能移动的内存页总数

## 内核如何管理虚拟内存区域

上面已经划分出了虚拟内存的各个部分，那么这些虚拟内存区域在内核中是如何表示的？这里引出一个结构体vm_area_struct

```c
struct vm_area_struct {

 unsigned long vm_start;  /* Our start address within vm_mm. */
 unsigned long vm_end;  /* The first byte after our end address
        within vm_mm. */
 /*
  * Access permissions of this VMA.
  */
 pgprot_t vm_page_prot;
 unsigned long vm_flags; 

 struct anon_vma *anon_vma; /* Serialized by page_table_lock */
    struct file * vm_file;  /* File we map to (can be NULL). */
 unsigned long vm_pgoff;  /* Offset (within vm_file) in PAGE_SIZE
        units */ 
 void * vm_private_data;  /* was vm_pte (shared mem) */
 /* Function pointers to deal with this struct. */
 const struct vm_operations_struct *vm_ops;
}
```

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%207.png)

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%208.png)

在内核中是通过一个struct vm_area_struct结构的双向链表将虚拟内存空间中的这些虚拟内存区域VMA串联起来的。vm_area_struct结构体中的vm_next和vm_prev指针分别指向VMA节点所在双向链表中的后继节点和前驱节点，内核中的这个VMA双向链表是有顺序的，所有VMA节点按照低地址到高地址的增长方向排序

双向链表中的最后一个VMA节点的vm_next指针指向NULL，双向链表的头指针存储在内存描述符struct mm_struct结构体中的mmap中，正是这个mmap串联起了整个虚拟内存空间中虚拟内存区域

```c
struct mm_struct {
    struct vm_area_struct *mmap;  /* list of VMAs */
}
```

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%209.png)

我们可以通过cat /proc/pid/maps或者pmap pid查看进程的虚拟内存空间布局以及其中包含的所有内存区域，这两个命令背后的实现原理就是通过遍历内核中的这个vm_area_struct双向链表实现的

内核中关于这些虚拟内存区域的操作除了遍历之外还有许多需要根据特定虚拟内存地址在虚拟内存空间中查找特定的虚拟内存区域，尤其在进程虚拟内存空间特别多的情况下，使用红黑树查找特定虚拟内存区域的时间复杂度O(LOGN)

所以在内核中同样的内存区域vm_area_struct会有两种组织形式，一种是双向链表用于高效遍历，一种是红黑树用于高效查找（每个VMA区域都是红黑树的一个节点，通过struct vm_area_struct结构中的vm_rb将自己连接到红黑树中）

```c
struct mm_struct {
     struct rb_root mm_rb;
}
```

![Untitled](linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%20c312376eae7f456993e3fbb4031b7185/Untitled%2010.png)

# 二进制文件如何映射到虚拟内存空间中

一个程序代码在编译之后会生成一个ELF格式的二进制文件，这个二进制文件中包含了程序运行时所需要的元信息，比如程序的机器码、程序中的全局变量等。这个ELF格式的二进制文件中的布局和虚拟内存空间布局是类似

- 磁盘文件中的段称为section，内存中的段称为segment
    
    

磁盘文件中的Section会在进程运行之前加载到内存中并映射到内存中Segment，通常是多个Section映射到一个Segment（比如.text和.rodata等一些只读Section，会被映射到内存的一个只读可执行的Segment中，而.data和.bss会被映射到一个具有读写权限的Segment中）

内核中完成这个映射过程的函数是load_elf_binary，这个函数的作用很大，加载内核的是它，启动第一个用户态进程init的是它，fork完了以后，调用exec运行一个二进制程序的也是它，当exec运行一个二进制程序的时候，除了解析ELF格式之外，另外一个重要的事情就是建立上述提到的内存映射

```c
static int load_elf_binary(struct linux_binprm *bprm)
{
      ...... 省略 ........
  // 设置虚拟内存空间中的内存映射区域起始地址 mmap_base
  setup_new_exec(bprm);

     ...... 省略 ........
  // 创建并初始化栈对应的 vm_area_struct 结构。
  // 设置 mm->start_stack 就是栈的起始地址也就是栈底，并将 mm->arg_start 是指向栈底的。
  retval = setup_arg_pages(bprm, randomize_stack_top(STACK_TOP),
         executable_stack);

     ...... 省略 ........
  // 将二进制文件中的代码部分映射到虚拟内存空间中
  error = elf_map(bprm->file, load_bias + vaddr, elf_ppnt,
        elf_prot, elf_flags, total_size);

     ...... 省略 ........
 // 创建并初始化堆对应的的 vm_area_struct 结构
 // 设置 current->mm->start_brk = current->mm->brk，设置堆的起始地址 start_brk，结束地址 brk。 起初两者相等表示堆是空的
  retval = set_brk(elf_bss, elf_brk, bss_prot);

     ...... 省略 ........
  // 将进程依赖的动态链接库 .so 文件映射到虚拟内存空间中的内存映射区域
  elf_entry = load_elf_interp(&loc->interp_elf_ex,
              interpreter,
              &interp_map_addr,
              load_bias, interp_elf_phdata);

     ...... 省略 ........
  // 初始化内存描述符 mm_struct
  current->mm->end_code = end_code;
  current->mm->start_code = start_code;
  current->mm->start_data = start_data;
  current->mm->end_data = end_data;
  current->mm->start_stack = bprm->p;

     ...... 省略 ........
}
```